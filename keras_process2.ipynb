{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "train = pd.read_csv('trains.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "for col in train.columns:\n",
    "    col_type = train[col].dtypes\n",
    "    min1 = train[col].min()\n",
    "    max1 = train[col].max()\n",
    "    if str(col_type)[:3] == 'int':\n",
    "        train[col] = train[col].astype(np.int16)\n",
    "    else:\n",
    "        if min1 > np.finfo(np.float16).min and max1 < np.finfo(np.float16).max:\n",
    "            train[col] = train[col].astype(np.float16)\n",
    "        elif min1 > np.finfo(np.float32).min and max1 < np.finfo(np.float32).max:\n",
    "            train[col] = train[col].astype(np.float32)\n",
    "        else:\n",
    "            train[col] = train[col].astype(np.float64)\n",
    "train_X = train.iloc[:,4:]\n",
    "train_Y = train.iloc[:,0:4]\n",
    "test_X = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "xscaler = MinMaxScaler()\n",
    "yscaler = MinMaxScaler()\n",
    "scaler_y = yscaler.fit_transform(train_Y)\n",
    "scaler_x = xscaler.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=226, activation='relu', input_dim=226))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=168, activation='relu'))\n",
    "    model.add(Dense(units=168, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=4, activation='linear'))\n",
    "    op = optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='mae', optimizer=op, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "class LRFinder(Callback):\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 1000\n",
    "epoch_size = len(train_X)\n",
    "\n",
    "lr_finder = LRFinder(min_lr=1e-3, \n",
    "                     max_lr=1e-2, \n",
    "                     steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                     epochs=epochs)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "ckpt_dir = './ckpt'\n",
    "ckpt_path = ckpt_dir + '/ResNetFinetuning_{epoch:02d}_valloss{val_loss:.2f}.hdf5'\n",
    "ckpt = keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 226)               904       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 168)               38136     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 168)               28392     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 168)               672       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               16900     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 104       \n",
      "=================================================================\n",
      "Total params: 156,635\n",
      "Trainable params: 155,547\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model11 = build_model()\n",
    "model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 729000 samples, validate on 81000 samples\n",
      "Epoch 1/100\n",
      "729000/729000 [==============================] - 41s 56us/step - loss: 0.1652 - mae: 0.1652 - val_loss: 0.2255 - val_mae: 0.2255\n",
      "Epoch 2/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0988 - mae: 0.0988 - val_loss: 0.1780 - val_mae: 0.1780\n",
      "Epoch 3/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0757 - mae: 0.0757 - val_loss: 0.1621 - val_mae: 0.1621\n",
      "Epoch 4/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0627 - mae: 0.0627 - val_loss: 0.1533 - val_mae: 0.1533\n",
      "Epoch 5/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0547 - mae: 0.0547 - val_loss: 0.1542 - val_mae: 0.1542\n",
      "Epoch 6/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0491 - mae: 0.0491 - val_loss: 0.1377 - val_mae: 0.1377\n",
      "Epoch 7/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0450 - mae: 0.0450 - val_loss: 0.1411 - val_mae: 0.1411\n",
      "Epoch 8/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0418 - mae: 0.0418 - val_loss: 0.1259 - val_mae: 0.1259\n",
      "Epoch 9/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0395 - mae: 0.0395 - val_loss: 0.1248 - val_mae: 0.1248\n",
      "Epoch 10/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0374 - mae: 0.0374 - val_loss: 0.1358 - val_mae: 0.1358\n",
      "Epoch 11/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0359 - mae: 0.0359 - val_loss: 0.1347 - val_mae: 0.1347\n",
      "Epoch 12/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0346 - mae: 0.0346 - val_loss: 0.1225 - val_mae: 0.1225\n",
      "Epoch 13/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0334 - mae: 0.0334 - val_loss: 0.1157 - val_mae: 0.1157\n",
      "Epoch 14/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0323 - mae: 0.0323 - val_loss: 0.1167 - val_mae: 0.1167\n",
      "Epoch 15/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0315 - mae: 0.0315 - val_loss: 0.1184 - val_mae: 0.1184\n",
      "Epoch 16/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.1170 - val_mae: 0.1170\n",
      "Epoch 17/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.1119 - val_mae: 0.1119\n",
      "Epoch 18/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0293 - mae: 0.0293 - val_loss: 0.1058 - val_mae: 0.1058\n",
      "Epoch 19/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0290 - mae: 0.0290 - val_loss: 0.1130 - val_mae: 0.1130\n",
      "Epoch 20/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0285 - mae: 0.0285 - val_loss: 0.1131 - val_mae: 0.1131\n",
      "Epoch 21/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0280 - mae: 0.0280 - val_loss: 0.1170 - val_mae: 0.1170\n",
      "Epoch 22/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0277 - mae: 0.0277 - val_loss: 0.1076 - val_mae: 0.1076\n",
      "Epoch 23/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0272 - mae: 0.0272 - val_loss: 0.1122 - val_mae: 0.1122\n",
      "Epoch 24/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0271 - mae: 0.0271 - val_loss: 0.1129 - val_mae: 0.1129\n",
      "Epoch 25/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0269 - mae: 0.0269 - val_loss: 0.1049 - val_mae: 0.1049\n",
      "Epoch 26/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0265 - mae: 0.0265 - val_loss: 0.1154 - val_mae: 0.1154\n",
      "Epoch 27/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0263 - mae: 0.0263 - val_loss: 0.1109 - val_mae: 0.1109\n",
      "Epoch 28/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0261 - mae: 0.0261 - val_loss: 0.1114 - val_mae: 0.1114\n",
      "Epoch 29/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0258 - mae: 0.0258 - val_loss: 0.0990 - val_mae: 0.0990\n",
      "Epoch 30/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0256 - mae: 0.0256 - val_loss: 0.1082 - val_mae: 0.1082\n",
      "Epoch 31/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.1159 - val_mae: 0.1159\n",
      "Epoch 32/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.1105 - val_mae: 0.1105\n",
      "Epoch 33/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0256 - mae: 0.0256 - val_loss: 0.1004 - val_mae: 0.1004\n",
      "Epoch 34/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0255 - mae: 0.0255 - val_loss: 0.1126 - val_mae: 0.1126\n",
      "Epoch 35/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0253 - mae: 0.0253 - val_loss: 0.1106 - val_mae: 0.1106\n",
      "Epoch 36/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0253 - mae: 0.0253 - val_loss: 0.1314 - val_mae: 0.1314\n",
      "Epoch 37/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.1144 - val_mae: 0.1144\n",
      "Epoch 38/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.1243 - val_mae: 0.1243\n",
      "Epoch 39/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.1231 - val_mae: 0.1231\n",
      "Epoch 40/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.1034 - val_mae: 0.1034\n",
      "Epoch 41/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.1099 - val_mae: 0.1099\n",
      "Epoch 42/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.1052 - val_mae: 0.1052\n",
      "Epoch 43/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0242 - mae: 0.0242 - val_loss: 0.1020 - val_mae: 0.1020\n",
      "Epoch 44/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.1059 - val_mae: 0.1059\n",
      "Epoch 45/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.1125 - val_mae: 0.1125\n",
      "Epoch 46/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.1153 - val_mae: 0.1153\n",
      "Epoch 47/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.1191 - val_mae: 0.1191\n",
      "Epoch 48/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.1109 - val_mae: 0.1109\n",
      "Epoch 49/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.1056 - val_mae: 0.1056\n",
      "Epoch 50/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.1030 - val_mae: 0.1030\n",
      "Epoch 51/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.1041 - val_mae: 0.1041\n",
      "Epoch 52/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.1051 - val_mae: 0.1051\n",
      "Epoch 53/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.1054 - val_mae: 0.1054\n",
      "Epoch 54/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0939 - val_mae: 0.0939\n",
      "Epoch 55/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.1132 - val_mae: 0.1132\n",
      "Epoch 56/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.1007 - val_mae: 0.1007\n",
      "Epoch 57/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.1040 - val_mae: 0.1040\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.1045 - val_mae: 0.1045\n",
      "Epoch 59/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.1055 - val_mae: 0.1055\n",
      "Epoch 60/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.1033 - val_mae: 0.1033\n",
      "Epoch 61/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0923 - val_mae: 0.0923\n",
      "Epoch 62/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.1042 - val_mae: 0.1042\n",
      "Epoch 63/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.1074 - val_mae: 0.1074\n",
      "Epoch 64/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0956 - val_mae: 0.0956\n",
      "Epoch 65/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.1016 - val_mae: 0.1016\n",
      "Epoch 66/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.1022 - val_mae: 0.1022\n",
      "Epoch 67/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.1000 - val_mae: 0.1000\n",
      "Epoch 68/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0949 - val_mae: 0.0949\n",
      "Epoch 69/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0975 - val_mae: 0.0975\n",
      "Epoch 70/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0981 - val_mae: 0.0981\n",
      "Epoch 71/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.1023 - val_mae: 0.1023\n",
      "Epoch 72/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0216 - mae: 0.0216 - val_loss: 0.0935 - val_mae: 0.0935\n",
      "Epoch 73/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.0949 - val_mae: 0.0949\n",
      "Epoch 74/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0972 - val_mae: 0.0972\n",
      "Epoch 75/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0897 - val_mae: 0.0897\n",
      "Epoch 76/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0215 - mae: 0.0215 - val_loss: 0.0966 - val_mae: 0.0966\n",
      "Epoch 77/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0993 - val_mae: 0.0993\n",
      "Epoch 78/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.1129 - val_mae: 0.1129\n",
      "Epoch 79/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0959 - val_mae: 0.0959\n",
      "Epoch 80/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0969 - val_mae: 0.0969\n",
      "Epoch 81/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0956 - val_mae: 0.0956\n",
      "Epoch 82/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0973 - val_mae: 0.0973\n",
      "Epoch 83/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.1126 - val_mae: 0.1126\n",
      "Epoch 84/100\n",
      "729000/729000 [==============================] - 20s 28us/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.1004 - val_mae: 0.1004\n",
      "Epoch 85/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.1025 - val_mae: 0.1025\n",
      "Epoch 86/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0937 - val_mae: 0.0937\n",
      "Epoch 87/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0937 - val_mae: 0.0937\n",
      "Epoch 88/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0983 - val_mae: 0.0983\n",
      "Epoch 89/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.0967 - val_mae: 0.0967\n",
      "Epoch 90/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0216 - mae: 0.0216 - val_loss: 0.0924 - val_mae: 0.0924\n",
      "Epoch 91/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.1027 - val_mae: 0.1027\n",
      "Epoch 92/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0217 - mae: 0.0217 - val_loss: 0.0941 - val_mae: 0.0941\n",
      "Epoch 93/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0216 - mae: 0.0216 - val_loss: 0.0946 - val_mae: 0.0946\n",
      "Epoch 94/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0218 - mae: 0.0218 - val_loss: 0.1010 - val_mae: 0.1010\n",
      "Epoch 95/100\n",
      "729000/729000 [==============================] - 19s 26us/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0910 - val_mae: 0.0910\n",
      "Epoch 96/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0222 - mae: 0.0222 - val_loss: 0.0973 - val_mae: 0.0973\n",
      "Epoch 97/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0223 - mae: 0.0223 - val_loss: 0.0910 - val_mae: 0.0910\n",
      "Epoch 98/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0220 - mae: 0.0220 - val_loss: 0.0988 - val_mae: 0.0988\n",
      "Epoch 99/100\n",
      "729000/729000 [==============================] - 20s 27us/step - loss: 0.0221 - mae: 0.0221 - val_loss: 0.0998 - val_mae: 0.0998\n",
      "Epoch 100/100\n",
      "729000/729000 [==============================] - 19s 27us/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0883 - val_mae: 0.0883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x273949bf080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model11 = build_model()\n",
    "model11.fit(scaler_x, scaler_y, epochs=100,callbacks=[ckpt, lr_finder], batch_size=500, validation_split = 0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_61_valloss0.09.hdf5')\n",
    "pred_test = model11.predict(xscaler.transform(test_X))\n",
    "y_pred1 = yscaler.inverse_transform(pred_test)\n",
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_54_valloss0.09.hdf5')\n",
    "pred_test = model11.predict(xscaler.transform(test_X))\n",
    "y_pred2 = yscaler.inverse_transform(pred_test)\n",
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_75_valloss0.09.hdf5')\n",
    "pred_test = model11.predict(xscaler.transform(test_X))\n",
    "y_pred3 = yscaler.inverse_transform(pred_test)\n",
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_100_valloss0.09.hdf5')\n",
    "pred_test = model11.predict(xscaler.transform(test_X))\n",
    "y_pred4 = yscaler.inverse_transform(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred1+y_pred2+y_pred3+y_pred4)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>255.740845</td>\n",
       "      <td>227.340714</td>\n",
       "      <td>132.363541</td>\n",
       "      <td>87.203773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>155.225677</td>\n",
       "      <td>133.928009</td>\n",
       "      <td>229.994293</td>\n",
       "      <td>99.849365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>146.288162</td>\n",
       "      <td>184.607651</td>\n",
       "      <td>270.082306</td>\n",
       "      <td>160.962891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>92.003288</td>\n",
       "      <td>223.876953</td>\n",
       "      <td>191.311127</td>\n",
       "      <td>79.020134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>267.545288</td>\n",
       "      <td>290.570435</td>\n",
       "      <td>242.225510</td>\n",
       "      <td>268.157135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     layer_1     layer_2     layer_3     layer_4\n",
       "0   0  255.740845  227.340714  132.363541   87.203773\n",
       "1   1  155.225677  133.928009  229.994293   99.849365\n",
       "2   2  146.288162  184.607651  270.082306  160.962891\n",
       "3   3   92.003288  223.876953  191.311127   79.020134\n",
       "4   4  267.545288  290.570435  242.225510  268.157135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample.iloc[:,1:] = y_pred\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"sample_sub1.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
