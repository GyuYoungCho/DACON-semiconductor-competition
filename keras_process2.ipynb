{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "train = pd.read_csv('trains.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "for col in train.columns:\n",
    "    col_type = train[col].dtypes\n",
    "    min1 = train[col].min()\n",
    "    max1 = train[col].max()\n",
    "    if str(col_type)[:3] == 'int':\n",
    "        train[col] = train[col].astype(np.int16)\n",
    "    else:\n",
    "        if min1 > np.finfo(np.float16).min and max1 < np.finfo(np.float16).max:\n",
    "            train[col] = train[col].astype(np.float16)\n",
    "        elif min1 > np.finfo(np.float32).min and max1 < np.finfo(np.float32).max:\n",
    "            train[col] = train[col].astype(np.float32)\n",
    "        else:\n",
    "            train[col] = train[col].astype(np.float64)\n",
    "train_X = train.iloc[:,4:]\n",
    "train_Y = train.iloc[:,0:4]\n",
    "test_X = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "xscaler = MinMaxScaler()\n",
    "yscaler = MinMaxScaler()\n",
    "scaler_y = yscaler.fit_transform(train_Y)\n",
    "scaler_x = xscaler.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras_radam import RAdam\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(Dense(units=226, activation=swish, input_dim=226))\n",
    "    model.add(Dense(units=4, activation='linear'))\n",
    "    op = optimizers.Nadam(learning_rate = 0.005, beta_1 = 0.9, beta_2 = 0.999)\n",
    "    model.compile(loss='mae', optimizer=op, metrics=['mae'])\n",
    "    return model\n",
    "def swish(x) :\n",
    "    return x * keras.activations.sigmoid(x)\n",
    "\n",
    "def mish(x) :\n",
    "    return x * keras.activations.tanh( keras.activations.softplus(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "class LRFinder(Callback):\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 1000\n",
    "epoch_size = len(train_X)\n",
    "\n",
    "lr_finder = LRFinder(min_lr=(1e-3), \n",
    "                     max_lr=1e-2, \n",
    "                     steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                     epochs=epochs)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "ckpt_dir = './ckpt'\n",
    "ckpt_path = ckpt_dir + '/ResNetFinetuning_{epoch:02d}_valloss{val_loss:.2f}.hdf5'\n",
    "ckpt = keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_133 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 226)               904       \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 226)               904       \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 226)               904       \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 226)               904       \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 226)               51302     \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 4)                 908       \n",
      "=================================================================\n",
      "Total params: 466,242\n",
      "Trainable params: 464,434\n",
      "Non-trainable params: 1,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model11 = build_model()\n",
    "model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 793800 samples, validate on 16200 samples\n",
      "Epoch 1/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 51.1573 - mae: 51.1573 - val_loss: 36.9046 - val_mae: 36.9046\n",
      "Epoch 2/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 17.0475 - mae: 17.0475 - val_loss: 23.5372 - val_mae: 23.5372\n",
      "Epoch 3/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 10.0130 - mae: 10.0130 - val_loss: 18.1417 - val_mae: 18.1417\n",
      "Epoch 4/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 7.7691 - mae: 7.7691 - val_loss: 17.0034 - val_mae: 17.0034\n",
      "Epoch 5/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 6.6670 - mae: 6.6670 - val_loss: 13.5097 - val_mae: 13.5097\n",
      "Epoch 6/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 6.0324 - mae: 6.0324 - val_loss: 14.2377 - val_mae: 14.2377\n",
      "Epoch 7/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 5.5494 - mae: 5.5494 - val_loss: 10.1178 - val_mae: 10.1178\n",
      "Epoch 8/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 5.2145 - mae: 5.2145 - val_loss: 11.5129 - val_mae: 11.5129\n",
      "Epoch 9/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.9994 - mae: 4.9994 - val_loss: 10.8442 - val_mae: 10.8442\n",
      "Epoch 10/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.8034 - mae: 4.8034 - val_loss: 10.3986 - val_mae: 10.3986\n",
      "Epoch 11/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.6044 - mae: 4.6044 - val_loss: 9.9006 - val_mae: 9.9006\n",
      "Epoch 12/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.4770 - mae: 4.4770 - val_loss: 9.2812 - val_mae: 9.2812\n",
      "Epoch 13/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.3435 - mae: 4.3435 - val_loss: 8.3816 - val_mae: 8.3816\n",
      "Epoch 14/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.2595 - mae: 4.2595 - val_loss: 6.7439 - val_mae: 6.7439\n",
      "Epoch 15/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.1148 - mae: 4.1148 - val_loss: 9.3125 - val_mae: 9.3125\n",
      "Epoch 16/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.0736 - mae: 4.0736 - val_loss: 7.9169 - val_mae: 7.9169\n",
      "Epoch 17/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 4.0109 - mae: 4.0109 - val_loss: 8.1963 - val_mae: 8.1963\n",
      "Epoch 18/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.8775 - mae: 3.8775 - val_loss: 6.9153 - val_mae: 6.9153\n",
      "Epoch 19/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.8570 - mae: 3.8570 - val_loss: 7.7397 - val_mae: 7.7397\n",
      "Epoch 20/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.8096 - mae: 3.8096 - val_loss: 6.9903 - val_mae: 6.9903\n",
      "Epoch 21/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.7674 - mae: 3.7674 - val_loss: 6.5679 - val_mae: 6.5679\n",
      "Epoch 22/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.6672 - mae: 3.6672 - val_loss: 9.7338 - val_mae: 9.7338\n",
      "Epoch 23/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.6522 - mae: 3.6522 - val_loss: 7.0861 - val_mae: 7.0861\n",
      "Epoch 24/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.6277 - mae: 3.6277 - val_loss: 7.8229 - val_mae: 7.8229\n",
      "Epoch 25/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.5310 - mae: 3.5310 - val_loss: 7.2487 - val_mae: 7.2487\n",
      "Epoch 26/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.4562 - mae: 3.4562 - val_loss: 6.6924 - val_mae: 6.6924\n",
      "Epoch 27/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.4852 - mae: 3.4852 - val_loss: 7.6025 - val_mae: 7.6025\n",
      "Epoch 28/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.4810 - mae: 3.4810 - val_loss: 6.4704 - val_mae: 6.4704\n",
      "Epoch 29/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.4048 - mae: 3.4048 - val_loss: 5.8915 - val_mae: 5.8915\n",
      "Epoch 30/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.3479 - mae: 3.3479 - val_loss: 6.0530 - val_mae: 6.0530\n",
      "Epoch 31/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.3366 - mae: 3.3366 - val_loss: 6.1568 - val_mae: 6.1568\n",
      "Epoch 32/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.3285 - mae: 3.3285 - val_loss: 6.4702 - val_mae: 6.4702\n",
      "Epoch 33/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.2578 - mae: 3.2578 - val_loss: 8.0541 - val_mae: 8.0541\n",
      "Epoch 34/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.2360 - mae: 3.2360 - val_loss: 5.9870 - val_mae: 5.9870\n",
      "Epoch 35/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.2305 - mae: 3.2305 - val_loss: 8.0404 - val_mae: 8.0404\n",
      "Epoch 36/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.2118 - mae: 3.2118 - val_loss: 6.7106 - val_mae: 6.7106\n",
      "Epoch 37/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.1634 - mae: 3.1634 - val_loss: 5.6329 - val_mae: 5.6329\n",
      "Epoch 38/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.1371 - mae: 3.1371 - val_loss: 6.9611 - val_mae: 6.9611\n",
      "Epoch 39/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.1553 - mae: 3.1553 - val_loss: 6.3041 - val_mae: 6.3041\n",
      "Epoch 40/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.0863 - mae: 3.0863 - val_loss: 7.6787 - val_mae: 7.6787\n",
      "Epoch 41/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 3.1377 - mae: 3.1377 - val_loss: 6.7561 - val_mae: 6.7561\n",
      "Epoch 42/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.0672 - mae: 3.0672 - val_loss: 7.3583 - val_mae: 7.3583\n",
      "Epoch 43/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.0399 - mae: 3.0399 - val_loss: 5.7808 - val_mae: 5.7808\n",
      "Epoch 44/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.0123 - mae: 3.0123 - val_loss: 6.2298 - val_mae: 6.2298\n",
      "Epoch 45/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 3.0414 - mae: 3.0414 - val_loss: 5.5791 - val_mae: 5.5791\n",
      "Epoch 46/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.9787 - mae: 2.9787 - val_loss: 5.0369 - val_mae: 5.0369\n",
      "Epoch 47/100\n",
      "793800/793800 [==============================] - ETA: 0s - loss: 2.9313 - mae: 2.931 - 16s 20us/step - loss: 2.9314 - mae: 2.9314 - val_loss: 6.7844 - val_mae: 6.7844\n",
      "Epoch 48/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.9595 - mae: 2.9595 - val_loss: 6.6217 - val_mae: 6.6217\n",
      "Epoch 49/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.8923 - mae: 2.8923 - val_loss: 6.3192 - val_mae: 6.3192\n",
      "Epoch 50/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.9249 - mae: 2.9249 - val_loss: 7.2292 - val_mae: 7.2292\n",
      "Epoch 51/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.8990 - mae: 2.8990 - val_loss: 8.4471 - val_mae: 8.4471 m\n",
      "Epoch 52/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.8557 - mae: 2.8557 - val_loss: 5.4086 - val_mae: 5.4086\n",
      "Epoch 53/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.8875 - mae: 2.8875 - val_loss: 5.9513 - val_mae: 5.9513\n",
      "Epoch 54/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.8496 - mae: 2.8496 - val_loss: 6.2469 - val_mae: 6.2469\n",
      "Epoch 55/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.8386 - mae: 2.8386 - val_loss: 6.4069 - val_mae: 6.4069\n",
      "Epoch 56/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.8594 - mae: 2.8594 - val_loss: 5.1449 - val_mae: 5.1449\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.7657 - mae: 2.7657 - val_loss: 6.0234 - val_mae: 6.0234\n",
      "Epoch 58/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.7652 - mae: 2.7652 - val_loss: 6.1558 - val_mae: 6.1558\n",
      "Epoch 59/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.7511 - mae: 2.7511 - val_loss: 9.6065 - val_mae: 9.6065\n",
      "Epoch 60/100\n",
      "793800/793800 [==============================] - 15s 20us/step - loss: 2.7396 - mae: 2.7396 - val_loss: 7.1350 - val_mae: 7.1350\n",
      "Epoch 61/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.6963 - mae: 2.6963 - val_loss: 5.1071 - val_mae: 5.1071\n",
      "Epoch 62/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.6617 - mae: 2.6617 - val_loss: 5.6800 - val_mae: 5.6800\n",
      "Epoch 63/100\n",
      "793800/793800 [==============================] - 15s 20us/step - loss: 2.6765 - mae: 2.6765 - val_loss: 6.9703 - val_mae: 6.9703\n",
      "Epoch 64/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.7006 - mae: 2.7006 - val_loss: 6.3833 - val_mae: 6.3833\n",
      "Epoch 65/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.5925 - mae: 2.5925 - val_loss: 5.8439 - val_mae: 5.8439\n",
      "Epoch 66/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.6659 - mae: 2.6659 - val_loss: 5.3347 - val_mae: 5.3348\n",
      "Epoch 67/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.6352 - mae: 2.6352 - val_loss: 6.6235 - val_mae: 6.6235\n",
      "Epoch 68/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.6364 - mae: 2.6364 - val_loss: 6.2075 - val_mae: 6.2075\n",
      "Epoch 69/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.6036 - mae: 2.6036 - val_loss: 5.9137 - val_mae: 5.9137\n",
      "Epoch 70/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.5401 - mae: 2.5401 - val_loss: 6.5098 - val_mae: 6.5098\n",
      "Epoch 71/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.5960 - mae: 2.5960 - val_loss: 5.9076 - val_mae: 5.9076\n",
      "Epoch 72/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.5243 - mae: 2.5243 - val_loss: 6.5920 - val_mae: 6.5920\n",
      "Epoch 73/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.5368 - mae: 2.5368 - val_loss: 5.0084 - val_mae: 5.0084\n",
      "Epoch 74/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.4803 - mae: 2.4803 - val_loss: 5.7562 - val_mae: 5.7562\n",
      "Epoch 75/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.5638 - mae: 2.5638 - val_loss: 6.2877 - val_mae: 6.2877\n",
      "Epoch 76/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.5224 - mae: 2.5224 - val_loss: 6.3650 - val_mae: 6.3650\n",
      "Epoch 77/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.4537 - mae: 2.4537 - val_loss: 4.9269 - val_mae: 4.9269\n",
      "Epoch 78/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.4821 - mae: 2.4821 - val_loss: 5.7959 - val_mae: 5.7959\n",
      "Epoch 79/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.4367 - mae: 2.4367 - val_loss: 6.3046 - val_mae: 6.3046\n",
      "Epoch 80/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.4441 - mae: 2.4441 - val_loss: 5.8966 - val_mae: 5.8966\n",
      "Epoch 81/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3889 - mae: 2.3889 - val_loss: 6.6598 - val_mae: 6.6598\n",
      "Epoch 82/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.4059 - mae: 2.4059 - val_loss: 6.5520 - val_mae: 6.5520\n",
      "Epoch 83/100\n",
      "793800/793800 [==============================] - 15s 20us/step - loss: 2.4374 - mae: 2.4374 - val_loss: 5.9949 - val_mae: 5.9949\n",
      "Epoch 84/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3476 - mae: 2.3476 - val_loss: 5.6089 - val_mae: 5.6089\n",
      "Epoch 85/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3758 - mae: 2.3758 - val_loss: 6.7022 - val_mae: 6.7022\n",
      "Epoch 86/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3703 - mae: 2.3703 - val_loss: 6.0393 - val_mae: 6.0393\n",
      "Epoch 87/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.4009 - mae: 2.4009 - val_loss: 5.9092 - val_mae: 5.9092\n",
      "Epoch 88/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3174 - mae: 2.3174 - val_loss: 6.3002 - val_mae: 6.3002\n",
      "Epoch 89/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3030 - mae: 2.3030 - val_loss: 5.6008 - val_mae: 5.6008\n",
      "Epoch 90/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.3580 - mae: 2.3580 - val_loss: 6.2747 - val_mae: 6.2747\n",
      "Epoch 91/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.3746 - mae: 2.3746 - val_loss: 5.3560 - val_mae: 5.3560\n",
      "Epoch 92/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2671 - mae: 2.2671 - val_loss: 6.3173 - val_mae: 6.3173\n",
      "Epoch 93/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2719 - mae: 2.2719 - val_loss: 5.6496 - val_mae: 5.6496\n",
      "Epoch 94/100\n",
      "793800/793800 [==============================] - 15s 19us/step - loss: 2.3061 - mae: 2.3061 - val_loss: 5.6272 - val_mae: 5.6272\n",
      "Epoch 95/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2774 - mae: 2.2774 - val_loss: 6.1453 - val_mae: 6.1453\n",
      "Epoch 96/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2844 - mae: 2.2844 - val_loss: 6.6364 - val_mae: 6.6364\n",
      "Epoch 97/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2590 - mae: 2.2590 - val_loss: 7.3106 - val_mae: 7.3106\n",
      "Epoch 98/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2693 - mae: 2.2693 - val_loss: 6.1365 - val_mae: 6.1365\n",
      "Epoch 99/100\n",
      "793800/793800 [==============================] - 15s 20us/step - loss: 2.2363 - mae: 2.2363 - val_loss: 6.4489 - val_mae: 6.4489\n",
      "Epoch 100/100\n",
      "793800/793800 [==============================] - 16s 20us/step - loss: 2.2394 - mae: 2.2394 - val_loss: 5.0378 - val_mae: 5.0378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d2b5bd2080>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model11 = build_model()\n",
    "model11.fit(train_X, train_Y, epochs=100,callbacks=[ckpt, lr_finder], batch_size=1000, validation_split = 0.02,shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = build_model()\n",
    "model11.fit(train_X, train_Y, epochs=100,callbacks=[ckpt, lr_finder], batch_size=1000, validation_split = 0.02,shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_46_valloss5.04.hdf5')\n",
    "pred_test = model11.predict(test_X)\n",
    "y_pred1 = pred_test\n",
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_77_valloss4.93.hdf5')\n",
    "pred_test = model11.predict(test_X)\n",
    "y_pred2 = pred_test\n",
    "# model11.load_weights(ckpt_dir +'/ResNetFinetuning_75_valloss0.09.hdf5')\n",
    "# pred_test = model11.predict(xscaler.transform(test_X))\n",
    "# y_pred3 = yscaler.inverse_transform(pred_test)\n",
    "model11.load_weights(ckpt_dir +'/ResNetFinetuning_73_valloss5.01.hdf5')\n",
    "pred_test = model11.predict(test_X)\n",
    "y_pred4 = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred1+y_pred2+y_pred4)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>256.817383</td>\n",
       "      <td>229.649979</td>\n",
       "      <td>132.002975</td>\n",
       "      <td>86.540886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>160.622375</td>\n",
       "      <td>125.623924</td>\n",
       "      <td>237.978683</td>\n",
       "      <td>99.343102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>147.094894</td>\n",
       "      <td>175.322449</td>\n",
       "      <td>274.911041</td>\n",
       "      <td>156.503922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>94.028076</td>\n",
       "      <td>232.451828</td>\n",
       "      <td>186.315689</td>\n",
       "      <td>83.999313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>275.882904</td>\n",
       "      <td>294.187347</td>\n",
       "      <td>242.459412</td>\n",
       "      <td>271.388794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     layer_1     layer_2     layer_3     layer_4\n",
       "0   0  256.817383  229.649979  132.002975   86.540886\n",
       "1   1  160.622375  125.623924  237.978683   99.343102\n",
       "2   2  147.094894  175.322449  274.911041  156.503922\n",
       "3   3   94.028076  232.451828  186.315689   83.999313\n",
       "4   4  275.882904  294.187347  242.459412  271.388794"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample.iloc[:,1:] = y_pred\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"sample_sub1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
